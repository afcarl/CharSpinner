{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@curiousily/making-a-predictive-keyboard-using-recurrent-neural-networks-tensorflow-for-hackers-part-v-3f238d824218\n",
    "# note: I trained on alice in wonderland, about 1/4 the length of his Nietzsche data set.\n",
    "# I also tested with \"seeds\" that come from his Nietzsche examples!\n",
    "# TODO history\n",
    "#    - what happens if you let it run further by itself? \n",
    "#   - Use  GRU instead of LSTM\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, GRU, Dropout,BatchNormalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import heapq\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "rcParams['figure.figsize'] = 12, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainP=True\n",
    "useNietzsche=True\n",
    "useCarroll=False\n",
    "\n",
    "#FILENAME=\"carroll.3.20\"   #for writing or reading\n",
    "#FILENAME=\"nietzsche.3.20\"   #for writing or reading\n",
    "LOGDIR=\"LOG\"\n",
    "RUNNAME=\"nietStep3Seq50\"\n",
    "FILENAME=LOGDIR + \"/\" + RUNNAME\n",
    "\n",
    "if not os.path.exists(LOGDIR):\n",
    "    os.makedirs(LOGDIR)\n",
    "\n",
    "SEQUENCE_LENGTH = 50\n",
    "EPOCHS=1\n",
    "step = 3   #skip this number of chars for generating new training sequences\n",
    "layer1size=128\n",
    "topN=1\n",
    "topNStartWord=3\n",
    "k_phraseLength=100\n",
    "\n",
    "k_layers=1\n",
    "k_bn=True\n",
    "k_batchsize=128\n",
    "\n",
    "k_stateful=False\n",
    "k_shuffle=True\n",
    "if (k_stateful) :\n",
    "    k_shuffle=False\n",
    "    \n",
    "k_validationSplit=.1\n",
    "    \n",
    "k_condNietzsche=[1,0]\n",
    "k_condCarroll=[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus 1 length: 372028\n",
      "NIETZSCHE CHARS:  [' ', \"'\", '(', ')', ',', '-', '.', '5', ':', ';', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "total cleaned corpus length is  372028\n",
      "unique chars:  38\n"
     ]
    }
   ],
   "source": [
    "import re as re\n",
    "def cleanText(text) :\n",
    "    # replace all numbers followed by an optional letter and then a dot (eg numbered paragraphs)                                    \n",
    "    text = re.sub(\"(^|\\W)\\d+[a-zA-Z]*($|\\W|\\.)\", \"\", text)\n",
    "    #escaped apotrophes                                                                                                             \n",
    "    text = text.replace('\\n', ' ').replace(\"\\'\", \"'\").replace(\"\\\"\",\"\").replace('[Illustration]',\"\").replace('*',\"\")\n",
    "    #repeated white space\n",
    "    text=re.sub('\\s{2,}',' ', text)\n",
    "    text=re.sub('â', 'a', text)\n",
    "    text=re.sub('æ', 'a', text)\n",
    "    text=re.sub('è', 'e', text)\n",
    "    text=re.sub('ï', 'i', text)\n",
    "    text=re.sub('ù', 'u', text)\n",
    "    text=re.sub('&c', 'etc', text)\n",
    "    text=re.sub('\\ufeff', '', text)\n",
    "    text=re.sub('‘', \"'\", text)\n",
    "    text=re.sub('’', \"'\", text)\n",
    "    text=re.sub('“', \"'\", text)\n",
    "    text=re.sub('”', \"'\", text)\n",
    "\n",
    "    # try to normalize Carroll text a bit more, although there are still way more contractions in carrll than nietzsche             \n",
    "    text=re.sub('!', \" \", text)\n",
    "    text=re.sub('\\?', \" \", text)\n",
    "#                                                                                                                                   \n",
    "    text=re.sub('--', \" \", text)\n",
    "    text=re.sub('_', \" \", text)\n",
    "\n",
    "    #repeated white space                                                                                                           \n",
    "    text=re.sub('\\s{2,}',' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "text1=\"\"\n",
    "text2=\"\"\n",
    "\n",
    "if useNietzsche :\n",
    "    path = 'nietzsche.txt'\n",
    "    text1 = cleanText(open(path, 'r', encoding='utf-8').read().lower())\n",
    "    print('corpus 1 length:', len(text1))\n",
    "    print(\"NIETZSCHE CHARS: \", sorted(list(set(text1))))\n",
    "    \n",
    "if useCarroll :\n",
    "    path = 'carroll.txt'\n",
    "    text2 = cleanText(open(path, 'r', encoding='utf-8').read().lower())\n",
    "    print('corpus 2 length:', len(text2))\n",
    "    print(\"CARROLL CHARS: \", sorted(list(set(text2))))\n",
    "\n",
    "text=text1+text2\n",
    "print('total cleaned corpus length is ', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "lenchars=len(chars)\n",
    "lenconditional=2\n",
    "lenAugmentedInput=lenchars+lenconditional\n",
    "\n",
    "#print(f'unique chars: {len(chars)}')\n",
    "print('unique chars: ', str(len(chars)))\n",
    "#chars\n",
    "#indices_char\n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num training examples:   123904\n",
      "num batches:   968.0\n",
      "num validation examples:   12288.0\n",
      "k_validationSplit:   0.09917355371900827\n"
     ]
    }
   ],
   "source": [
    "#CREAT TRAINING DATA\n",
    "# cut the corpus into chunks of 40 characters, spacing the sequences by 3 characters\n",
    "# Additionally, we will store the next character (the one we need to predict) for every sequence\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "cond_input=[]\n",
    "\n",
    "#grab as many full batches as possible, ignoring partial batch left over\n",
    "samples1= (int(len(text1)/step) - SEQUENCE_LENGTH)-(int(len(text1)/step) - SEQUENCE_LENGTH)%k_batchsize\n",
    "samples2= (int(len(text2)/step) - SEQUENCE_LENGTH)-(int(len(text2)/step) - SEQUENCE_LENGTH)%k_batchsize\n",
    "\n",
    "if useNietzsche :\n",
    "    for i in range(0, samples1*step, step):\n",
    "        sentences.append(text1[i: i + SEQUENCE_LENGTH])\n",
    "        next_chars.append(text1[i + SEQUENCE_LENGTH])\n",
    "        cond_input.append(k_condNietzsche)\n",
    "\n",
    "if useCarroll :\n",
    "    for j in range(0, samples2*step, step):\n",
    "        sentences.append(text2[j: j + SEQUENCE_LENGTH])\n",
    "        next_chars.append(text2[j + SEQUENCE_LENGTH])\n",
    "        cond_input.append(k_condCarroll)\n",
    "\n",
    "#print(f'num training examples: {len(sentences)}')\n",
    "print('num training examples:  ', str(len(sentences)))\n",
    "print('num batches:  ', str(len(sentences)/k_batchsize))\n",
    "\n",
    "if (True) : # (k_stateful) : # ALWAYS do full batches for training and testing (required for k_stateful, anyway)\n",
    "    # adjust the validation split so that it has an integer number of batches of size k_batchsize\n",
    "    numvexamples=k_validationSplit*len(sentences)  #target number\n",
    "    numvexamples=numvexamples-numvexamples%k_batchsize #divisible by batch size\n",
    "    k_validationSplit=numvexamples/len(sentences) #adjusted split number for fit()\n",
    "\n",
    "    print('num validation examples:  ', str(numvexamples))\n",
    "    print('k_validationSplit:  ', str(k_validationSplit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(123904*k_validationSplit)%128\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILENAME  :  LOG/nietStep3Seq60\n",
      "RUNNAME  :  nietStep3Seq60\n",
      "SEQUENCE_LENGTH  :  5\n",
      "EPOCHS  :  1\n",
      "step  :  3\n",
      "layer1size  :  128\n",
      "k_layers  :  1\n",
      "k_bn  :  True\n",
      "k_batchsize  :  128\n",
      "k_stateful  :  False\n",
      "k_shuffle  :  True\n",
      "k_validationSplit  :  0.09917355371900827\n"
     ]
    }
   ],
   "source": [
    "# save parameters of run\n",
    "param={'FILENAME': FILENAME, 'RUNNAME': RUNNAME, 'SEQUENCE_LENGTH': SEQUENCE_LENGTH, 'EPOCHS': EPOCHS, 'step': step, 'layer1size': layer1size, 'k_layers': k_layers, 'k_bn': k_bn, 'k_batchsize': k_batchsize, 'k_stateful': k_stateful, 'k_shuffle': k_shuffle, 'k_validationSplit': k_validationSplit}  \n",
    "\n",
    "with open(FILENAME + '.params.pkl', 'wb') as f:  \n",
    "    pickle.dump(param, f)\n",
    "    \n",
    "#CHECK: get parameters from training param file\n",
    "with open(FILENAME + '.params.pkl', 'rb') as f:  \n",
    "    param = pickle.load(f)\n",
    "# and PRINT\n",
    "for key, value in param.items():\n",
    "    print(key,  \" : \", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate features and labels - one-host versions of the input and prediction vectors\n",
    "\n",
    "X = np.zeros((len(sentences), SEQUENCE_LENGTH, lenAugmentedInput), dtype=np.bool)  #x[sample_index][one-hot array]\n",
    "y = np.zeros((len(sentences), lenchars), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "        X[i, t,-lenconditional:] = cond_input[i] # set conditional bit on last charcter in each training sample\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_validationSplit*len(y)/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of first input vector, first character vector x is  40\n",
      "a character in a sentice is  [ True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True False]\n",
      "(123904, 5, 40)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"len of first input vector, first character vector x is \", str(len(X[0][0])))\n",
    "print(\"a character in a sentice is \", X[1410][0] )\n",
    "\n",
    "print(str(X.shape))   #training_samples, SEQUENCE_LENGTH, lenAugmentedInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LST layer with 128 neurons\n",
    "# takes a shape which is \n",
    "model = Sequential()\n",
    "\n",
    "if (k_layers==1) :\n",
    "    model.add(LSTM(layer1size,  batch_size=k_batchsize, stateful=k_stateful, input_shape=(SEQUENCE_LENGTH, lenAugmentedInput)))\n",
    "else :\n",
    "    model.add(LSTM(layer1size,   batch_size=k_batchsize, stateful=k_stateful, return_sequences=True, input_shape=(SEQUENCE_LENGTH, lenAugmentedInput)))\n",
    "    \n",
    "\n",
    "#FAIL model.add(LSTM(layer1size, dropout=0.25, recurrent_dropout=0.25, input_shape=(SEQUENCE_LENGTH, lenAugmentedInput)))\n",
    "#FAIL model.add(GRU(layer1size,  input_shape=(SEQUENCE_LENGTH, lenAugmentedInput), kernel_regularizer=regularizers.l2(0.1), recurrent_regularizer=regularizers.l2(0.1), bias_regularizer=regularizers.l2(0.1), activity_regularizer=regularizers.l2(0.1)))\n",
    "\n",
    "#model.add(Dense(lenchars, kernel_regularizer=regularizers.l2(0.1),  bias_regularizer=regularizers.l2(0.1), activity_regularizer=regularizers.l2(0.1)))\n",
    "#model.add(Dense(lenchars, kernel_regularizer=regularizers.l2(0.1),  bias_regularizer=regularizers.l2(0.1), activity_regularizer=regularizers.l2(0.1)))\n",
    "#\n",
    "\n",
    "if (k_bn) : \n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "if (k_layers==2) :\n",
    "    model.add(LSTM(layer1size))\n",
    "    \n",
    "model.add(Dense(lenchars))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 111616 samples, validate on 12288 samples\n",
      "Epoch 1/1\n",
      "111488/111616 [============================>.] - ETA: 0s - loss: 2.0999 - acc: 0.3752Epoch 00000: saving model to LOG/nietStep3Seq60.cp-00-0.45.hdf5\n",
      "111616/111616 [==============================] - 20s - loss: 2.0997 - acc: 0.3753 - val_loss: 1.8755 - val_acc: 0.4456\n"
     ]
    }
   ],
   "source": [
    "# Train. Validate with 5% of the examples\n",
    "\n",
    "if trainP :\n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    # checkpoint\n",
    "    filepath= FILENAME + \".cp-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1)\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "    history = model.fit(X, y, validation_split=k_validationSplit, batch_size=k_batchsize, epochs=EPOCHS, shuffle=k_shuffle, callbacks=callbacks_list).history\n",
    "\n",
    "    #Save (How does this work ??)\n",
    "    model.save(FILENAME + '.keras_model.h5')\n",
    "    pickle.dump(history, open(FILENAME + '.history.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (keras)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
