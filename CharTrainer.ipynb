{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@curiousily/making-a-predictive-keyboard-using-recurrent-neural-networks-tensorflow-for-hackers-part-v-3f238d824218\n",
    "# note: I trained on alice in wonderland, about 1/4 the length of his Nietzsche data set.\n",
    "# I also tested with \"seeds\" that come from his Nietzsche examples!\n",
    "# TODO history\n",
    "#    - what happens if you let it run further by itself? \n",
    "#   - Use  GRU instead of LSTM\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, GRU, Dropout,BatchNormalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import heapq\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "rcParams['figure.figsize'] = 12, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainP=True\n",
    "useNietzsche=False\n",
    "useCarroll=False\n",
    "useShakespeare=True\n",
    "\n",
    "LOGDIR=\"LOG\"\n",
    "RUNNAME=\"minishakespeareStep3Seq50\"\n",
    "FILENAME=LOGDIR + \"/\" + RUNNAME\n",
    "\n",
    "if not os.path.exists(LOGDIR):\n",
    "    os.makedirs(LOGDIR)\n",
    "\n",
    "SEQUENCE_LENGTH = 50\n",
    "EPOCHS=15\n",
    "step = 3   #skip this number of chars for generating new training sequences\n",
    "layer1size=128\n",
    "topN=1\n",
    "topNStartWord=3\n",
    "k_phraseLength=100\n",
    "\n",
    "k_layers=2\n",
    "k_bn=True\n",
    "k_batchsize=128\n",
    "k_lr=.005\n",
    "\n",
    "k_stateful=False\n",
    "k_shuffle=True\n",
    "if (k_stateful) :\n",
    "    k_shuffle=False\n",
    "    \n",
    "k_validationSplit=.1\n",
    "    \n",
    "k_condNietzsche=[1,0,0]\n",
    "k_condCarroll=[0,1,0]\n",
    "k_condShakespeare=[0,0,1]\n",
    "lenconditional=len(k_condNietzsche) # the are all the same length, of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus 3 length: 1236968\n",
      "SHAKESPEARE CHARS:  ['\\n', ' ', '$', \"'\", ',', '-', '.', ':', ';', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "total cleaned corpus length is  1236968\n",
      "unique chars:  37\n"
     ]
    }
   ],
   "source": [
    "import re as re\n",
    "def cleanText(text) :\n",
    "    # replace all numbers followed by an optional letter and then a dot (eg numbered paragraphs)                                    \n",
    "    text = re.sub(\"(^|\\W)\\d+[a-zA-Z]*($|\\W|\\.)\", \"\", text)\n",
    "    #escaped apotrophes                                                                                                             \n",
    "    #text = text.replace('\\n', ' ').replace(\"\\'\", \"'\").replace(\"\\\"\",\"\").replace('[Illustration]',\"\").replace('*',\"\")\n",
    "    text = text.replace(\"\\'\", \"'\").replace(\"\\\"\",\"\").replace('[Illustration]',\"\").replace('*',\"\")\n",
    "    #repeated white space\n",
    "    text=re.sub('â', 'a', text)\n",
    "    text=re.sub('æ', 'a', text)\n",
    "    text=re.sub('è', 'e', text)\n",
    "    text=re.sub('ï', 'i', text)\n",
    "    text=re.sub('ù', 'u', text)\n",
    "    text=re.sub('&c', 'etc', text)\n",
    "    text=re.sub('\\ufeff', '', text)\n",
    "    text=re.sub('‘', \"'\", text)\n",
    "    text=re.sub('’', \"'\", text)\n",
    "    text=re.sub('“', \"'\", text)\n",
    "    text=re.sub('”', \"'\", text)\n",
    "\n",
    "    # try to normalize Carroll text a bit more, although there are still way more contractions in carrll than nietzsche             \n",
    "    text=re.sub('!', \" \", text)\n",
    "    text=re.sub('\\?', \" \", text)\n",
    "#                                                                                                                                   \n",
    "    text=re.sub('--', \" \", text)\n",
    "    text=re.sub('_', \" \", text)\n",
    "\n",
    "    #repeated white space                                                                                                           \n",
    "    text=re.sub('\\s{2,}',' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "text1=\"\"\n",
    "text2=\"\"\n",
    "text3=\"\"\n",
    "\n",
    "if useNietzsche :\n",
    "    path = 'nietzsche.txt'\n",
    "    text1 = cleanText(open(path, 'r', encoding='utf-8').read().lower())\n",
    "    print('corpus 1 length:', len(text1))\n",
    "    print(\"NIETZSCHE CHARS: \", sorted(list(set(text1))))\n",
    "    \n",
    "if useCarroll :\n",
    "    path = 'carroll.txt'\n",
    "    text2 = cleanText(open(path, 'r', encoding='utf-8').read().lower())\n",
    "    print('corpus 2 length:', len(text2))\n",
    "    print(\"CARROLL CHARS: \", sorted(list(set(text2))))\n",
    "\n",
    "if useShakespeare :\n",
    "    path = 'minishakespeare.txt'\n",
    "    text3 = cleanText(open(path, 'r', encoding='utf-8').read().lower())\n",
    "    print('corpus 3 length:', len(text3))\n",
    "    print(\"SHAKESPEARE CHARS: \", sorted(list(set(text3))))\n",
    "    \n",
    "text=text1+text2+text3\n",
    "print('total cleaned corpus length is ', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "lenchars=len(chars)\n",
    "lenAugmentedInput=lenchars+lenconditional\n",
    "\n",
    "#print(f'unique chars: {len(chars)}')\n",
    "print('unique chars: ', str(len(chars)))\n",
    "#chars\n",
    "#indices_char\n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num training examples:   412288\n",
      "num batches:   3221.0\n",
      "num validation examples:   41216.0\n",
      "k_validationSplit:   0.0999689537410742\n"
     ]
    }
   ],
   "source": [
    "#CREAT TRAINING DATA\n",
    "# cut the corpus into chunks of 40 characters, spacing the sequences by 3 characters\n",
    "# Additionally, we will store the next character (the one we need to predict) for every sequence\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "cond_input=[]\n",
    "\n",
    "#grab as many full batches as possible, ignoring partial batch left over\n",
    "samples1= (int(len(text1)/step) - SEQUENCE_LENGTH)-(int(len(text1)/step) - SEQUENCE_LENGTH)%k_batchsize\n",
    "samples2= (int(len(text2)/step) - SEQUENCE_LENGTH)-(int(len(text2)/step) - SEQUENCE_LENGTH)%k_batchsize\n",
    "samples3= (int(len(text3)/step) - SEQUENCE_LENGTH)-(int(len(text3)/step) - SEQUENCE_LENGTH)%k_batchsize\n",
    "\n",
    "if useNietzsche :\n",
    "    for i in range(0, samples1*step, step):\n",
    "        sentences.append(text1[i: i + SEQUENCE_LENGTH])\n",
    "        next_chars.append(text1[i + SEQUENCE_LENGTH])\n",
    "        cond_input.append(k_condNietzsche)\n",
    "\n",
    "if useCarroll :\n",
    "    for j in range(0, samples2*step, step):\n",
    "        sentences.append(text2[j: j + SEQUENCE_LENGTH])\n",
    "        next_chars.append(text2[j + SEQUENCE_LENGTH])\n",
    "        cond_input.append(k_condCarroll)\n",
    "\n",
    "if useShakespeare :\n",
    "    for k in range(0, samples3*step, step):\n",
    "        sentences.append(text3[k: k + SEQUENCE_LENGTH])\n",
    "        next_chars.append(text3[k + SEQUENCE_LENGTH])\n",
    "        cond_input.append(k_condShakespeare)\n",
    "\n",
    "\n",
    "#print(f'num training examples: {len(sentences)}')\n",
    "print('num training examples:  ', str(len(sentences)))\n",
    "print('num batches:  ', str(len(sentences)/k_batchsize))\n",
    "\n",
    "if (True) : # (k_stateful) : # ALWAYS do full batches for training and testing (required for k_stateful, anyway)\n",
    "    # adjust the validation split so that it has an integer number of batches of size k_batchsize\n",
    "    numvexamples=k_validationSplit*len(sentences)  #target number\n",
    "    numvexamples=numvexamples-numvexamples%k_batchsize #divisible by batch size\n",
    "    k_validationSplit=numvexamples/len(sentences) #adjusted split number for fit()\n",
    "\n",
    "    print('num validation examples:  ', str(numvexamples))\n",
    "    print('k_validationSplit:  ', str(k_validationSplit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILENAME  :  LOG/minishakespeareStep3Seq50\n",
      "RUNNAME  :  minishakespeareStep3Seq50\n",
      "SEQUENCE_LENGTH  :  5\n",
      "EPOCHS  :  1\n",
      "step  :  3\n",
      "layer1size  :  128\n",
      "k_layers  :  3\n",
      "k_bn  :  True\n",
      "k_batchsize  :  128\n",
      "k_lr  :  0.005\n",
      "k_stateful  :  False\n",
      "k_shuffle  :  True\n",
      "k_validationSplit  :  0.0999689537410742\n",
      "chars  :  ['\\n', ' ', '$', \"'\", ',', '-', '.', ':', ';', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# save parameters of run\n",
    "param={'FILENAME': FILENAME, 'RUNNAME': RUNNAME, 'SEQUENCE_LENGTH': SEQUENCE_LENGTH, 'EPOCHS': EPOCHS, 'step': step, 'layer1size': layer1size, 'k_layers': k_layers, 'k_bn': k_bn, 'k_batchsize': k_batchsize, 'k_lr': k_lr, 'k_stateful': k_stateful, 'k_shuffle': k_shuffle, 'k_validationSplit': k_validationSplit, 'chars': chars}  \n",
    "\n",
    "with open(FILENAME + '.params.pkl', 'wb') as f:  \n",
    "    pickle.dump(param, f)\n",
    "    \n",
    "#CHECK: get parameters from training param file\n",
    "with open(FILENAME + '.params.pkl', 'rb') as f:  \n",
    "    param = pickle.load(f)\n",
    "# and PRINT\n",
    "for key, value in param.items():\n",
    "    print(key,  \" : \", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate features and labels - one-host versions of the input and prediction vectors\n",
    "\n",
    "X = np.zeros((len(sentences), SEQUENCE_LENGTH, lenAugmentedInput), dtype=np.bool)  #x[sample_index][one-hot array]\n",
    "y = np.zeros((len(sentences), lenchars), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "        X[i, t,-lenconditional:] = cond_input[i] # set conditional bit on last charcter in each training sample\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_validationSplit*len(y)/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of first input vector, first character vector x is  40\n",
      "a character in a sentice is  [False False False False False False False False False False False False\n",
      " False False False  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True]\n",
      "(412288, 5, 40)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"len of first input vector, first character vector x is \", str(len(X[0][0])))\n",
    "print(\"a character in a sentice is \", X[1410][0] )\n",
    "\n",
    "print(str(X.shape))   #training_samples, SEQUENCE_LENGTH, lenAugmentedInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(k_layers) :\n",
    "    print( str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LST layer with 128 neurons\n",
    "# takes a shape which is \n",
    "model = Sequential()\n",
    "\n",
    "for i in range(k_layers) :\n",
    "    #if last layer, don't return sequence\n",
    "    if i==(k_layers-1) :\n",
    "        model.add(LSTM(layer1size,  batch_size=k_batchsize, stateful=k_stateful, input_shape=(SEQUENCE_LENGTH, lenAugmentedInput)))\n",
    "    else :\n",
    "        model.add(LSTM(layer1size,   batch_size=k_batchsize, stateful=k_stateful, return_sequences=True, input_shape=(SEQUENCE_LENGTH, lenAugmentedInput)))\n",
    "        \n",
    "    #FAIL model.add(LSTM(layer1size, dropout=0.25, recurrent_dropout=0.25, input_shape=(SEQUENCE_LENGTH, lenAugmentedInput)))\n",
    "    #FAIL model.add(GRU(layer1size,  input_shape=(SEQUENCE_LENGTH, lenAugmentedInput), kernel_regularizer=regularizers.l2(0.1), recurrent_regularizer=regularizers.l2(0.1), bias_regularizer=regularizers.l2(0.1), activity_regularizer=regularizers.l2(0.1)))\n",
    "\n",
    "    if (k_bn) : \n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dense(lenchars, kernel_regularizer=regularizers.l2(0.1),  bias_regularizer=regularizers.l2(0.1), activity_regularizer=regularizers.l2(0.1)))\n",
    "#model.add(Dense(lenchars, kernel_regularizer=regularizers.l2(0.1),  bias_regularizer=regularizers.l2(0.1), activity_regularizer=regularizers.l2(0.1)))\n",
    "model.add(Dense(lenchars))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 371072 samples, validate on 41216 samples\n",
      "Epoch 1/1\n",
      "370944/371072 [============================>.] - ETA: 0s - loss: 1.8712 - acc: 0.4393Epoch 00000: saving model to LOG/minishakespeareStep3Seq50.cp-00-0.46.hdf5\n",
      "371072/371072 [==============================] - 284s - loss: 1.8711 - acc: 0.4393 - val_loss: 1.7877 - val_acc: 0.4593\n"
     ]
    }
   ],
   "source": [
    "# Train. Validate with 5% of the examples\n",
    "\n",
    "if trainP :\n",
    "    optimizer = RMSprop(lr=k_lr)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    # checkpoint\n",
    "    filepath= FILENAME + \".cp-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1)\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "    history = model.fit(X, y, validation_split=k_validationSplit, batch_size=k_batchsize, epochs=EPOCHS, shuffle=k_shuffle, callbacks=callbacks_list).history\n",
    "\n",
    "    #Save (How does this work ??)\n",
    "    model.save(FILENAME + '.keras_model.h5')\n",
    "    pickle.dump(history, open(FILENAME + '.history.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (keras)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
